{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3 — Classification: S4 vs Transformer\n",
    "\n",
    "In this notebook we train **S4** and a **Transformer** baseline on the same synthetic classification task and compare:\n",
    "\n",
    "- **Test accuracy**\n",
    "- **Parameter count**\n",
    "- **Training speed** (time per epoch)\n",
    "\n",
    "## Task: Delayed-XOR\n",
    "\n",
    "A sequence of length $L=256$ with 3 channels:\n",
    "\n",
    "| Channel | Content |\n",
    "|---------|--------|\n",
    "| 0 | random bit (0/1) placed at position 0, rest is 0 |\n",
    "| 1 | random bit (0/1) placed at position 128 (halfway), rest is 0 |\n",
    "| 2 | Gaussian noise (distractor) |\n",
    "\n",
    "**Label** = XOR of the two bits = `ch0[0] ⊕ ch1[128]` → binary classification.\n",
    "\n",
    "Why is this hard? The model must **remember** a signal from step 0 until step 128 — a 128-step long-range dependency. SSMs are built for this; Transformers rely on attention to bridge the gap.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, time\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), \"..\") if \"tutorials\" in os.getcwd() else os.getcwd())\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate the Delayed-XOR Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_delayed_xor(n_samples, seq_len=256, delay=128):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        X: (n_samples, seq_len, 3) float tensor\n",
    "        y: (n_samples,) long tensor — 0 or 1\n",
    "    \"\"\"\n",
    "    X = torch.zeros(n_samples, seq_len, 3)\n",
    "    bits_a = torch.randint(0, 2, (n_samples,))\n",
    "    bits_b = torch.randint(0, 2, (n_samples,))\n",
    "    X[:, 0, 0] = bits_a.float()\n",
    "    X[:, delay, 1] = bits_b.float()\n",
    "    X[:, :, 2] = torch.randn(n_samples, seq_len) * 0.1  # noise distractor\n",
    "    y = (bits_a ^ bits_b).long()\n",
    "    return X, y\n",
    "\n",
    "N_TRAIN, N_TEST = 4000, 1000\n",
    "SEQ_LEN = 256\n",
    "\n",
    "X_train, y_train = make_delayed_xor(N_TRAIN, SEQ_LEN)\n",
    "X_test, y_test   = make_delayed_xor(N_TEST, SEQ_LEN)\n",
    "\n",
    "print(f\"Train: X={X_train.shape}, y={y_train.shape}, class balance: {y_train.float().mean():.2f}\")\n",
    "print(f\"Test:  X={X_test.shape},  y={y_test.shape},  class balance: {y_test.float().mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize one sample\n",
    "idx = 0\n",
    "fig, axes = plt.subplots(3, 1, figsize=(10, 4), sharex=True)\n",
    "for ch, name in enumerate([\"Bit A (pos 0)\", \"Bit B (pos 128)\", \"Noise\"]):\n",
    "    axes[ch].plot(X_train[idx, :, ch].numpy(), linewidth=1)\n",
    "    axes[ch].set_ylabel(name, fontsize=9)\n",
    "axes[-1].set_xlabel(\"Time step\")\n",
    "fig.suptitle(f\"Sample {idx}: label = {y_train[idx].item()} (XOR)\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_ds = TensorDataset(X_train, y_train)\n",
    "test_ds  = TensorDataset(X_test, y_test)\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dl  = DataLoader(test_ds,  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Models\n",
    "\n",
    "### S4 model\n",
    "Our `S4SequenceModel` with `task=\"classification\"` — mean-pools over time and passes through a linear head.\n",
    "\n",
    "### Transformer baseline\n",
    "A standard `nn.TransformerEncoder` with learned positional embeddings, followed by mean-pooling and a classifier head. We match `d_model` and `n_layers` for a fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from s4_lib import S4SequenceModel, get_ssm_param_groups\n",
    "\n",
    "D_MODEL = 64\n",
    "N_LAYERS = 4\n",
    "N_CLASSES = 2\n",
    "\n",
    "# ---- S4 model ----\n",
    "s4_model = S4SequenceModel(\n",
    "    d_input=3, d_model=D_MODEL, d_output=N_CLASSES,\n",
    "    n_layers=N_LAYERS, d_state=64, dropout=0.1,\n",
    "    task=\"classification\",\n",
    ").to(DEVICE)\n",
    "\n",
    "s4_params = sum(p.numel() for p in s4_model.parameters())\n",
    "print(f\"S4 model params: {s4_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "    \"\"\"Transformer encoder + positional embedding + mean-pool + classifier.\"\"\"\n",
    "    def __init__(self, d_input, d_model, n_layers, n_classes,\n",
    "                 max_len=512, nhead=4, dim_ff=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_input, d_model)\n",
    "        self.pos_emb = nn.Parameter(torch.randn(1, max_len, d_model) * 0.02)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=dim_ff,\n",
    "            dropout=dropout, batch_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.head = nn.Linear(d_model, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L, _ = x.shape\n",
    "        x = self.proj(x) + self.pos_emb[:, :L, :]\n",
    "        x = self.encoder(x)\n",
    "        x = x.mean(dim=1)\n",
    "        return self.head(x)\n",
    "\n",
    "tf_model = TransformerClassifier(\n",
    "    d_input=3, d_model=D_MODEL, n_layers=N_LAYERS, n_classes=N_CLASSES,\n",
    "    max_len=SEQ_LEN, nhead=4, dim_ff=D_MODEL*4, dropout=0.1,\n",
    ").to(DEVICE)\n",
    "\n",
    "tf_params = sum(p.numel() for p in tf_model.parameters())\n",
    "print(f\"Transformer model params: {tf_params:,}\")\n",
    "print(f\"Ratio (Transformer / S4): {tf_params/s4_params:.2f}×\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training\n",
    "\n",
    "We train both models for 30 epochs with the same optimizer settings (AdamW, lr=0.004). S4 gets a separate SSM learning rate of 0.001 as recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, loader, criterion, device):\n",
    "    model.train()\n",
    "    total_loss, correct, n = 0.0, 0, 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        correct += (logits.argmax(-1) == yb).sum().item()\n",
    "        n += xb.size(0)\n",
    "    return total_loss / n, correct / n\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, correct, n = 0.0, 0, 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        correct += (logits.argmax(-1) == yb).sum().item()\n",
    "        n += xb.size(0)\n",
    "    return total_loss / n, correct / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 30\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# S4 optimizer (dual LR)\n",
    "s4_opt = torch.optim.AdamW(\n",
    "    get_ssm_param_groups(s4_model, lr=0.004, ssm_lr=0.001, weight_decay=0.01)\n",
    ")\n",
    "\n",
    "# Transformer optimizer\n",
    "tf_opt = torch.optim.AdamW(tf_model.parameters(), lr=0.004, weight_decay=0.01)\n",
    "\n",
    "# ---- Train S4 ----\n",
    "s4_hist = {\"train_loss\": [], \"test_loss\": [], \"train_acc\": [], \"test_acc\": [], \"epoch_time\": []}\n",
    "print(\"Training S4...\")\n",
    "for ep in range(N_EPOCHS):\n",
    "    t0 = time.time()\n",
    "    tr_loss, tr_acc = train_one_epoch(s4_model, s4_opt, train_dl, criterion, DEVICE)\n",
    "    te_loss, te_acc = evaluate(s4_model, test_dl, criterion, DEVICE)\n",
    "    dt = time.time() - t0\n",
    "    s4_hist[\"train_loss\"].append(tr_loss)\n",
    "    s4_hist[\"test_loss\"].append(te_loss)\n",
    "    s4_hist[\"train_acc\"].append(tr_acc)\n",
    "    s4_hist[\"test_acc\"].append(te_acc)\n",
    "    s4_hist[\"epoch_time\"].append(dt)\n",
    "    if (ep + 1) % 5 == 0:\n",
    "        print(f\"  [S4] Epoch {ep+1:2d}/{N_EPOCHS}: loss={tr_loss:.4f}  \"\n",
    "              f\"acc={tr_acc:.3f}  test_acc={te_acc:.3f}  ({dt:.1f}s)\")\n",
    "\n",
    "# ---- Train Transformer ----\n",
    "tf_hist = {\"train_loss\": [], \"test_loss\": [], \"train_acc\": [], \"test_acc\": [], \"epoch_time\": []}\n",
    "print(\"\\nTraining Transformer...\")\n",
    "for ep in range(N_EPOCHS):\n",
    "    t0 = time.time()\n",
    "    tr_loss, tr_acc = train_one_epoch(tf_model, tf_opt, train_dl, criterion, DEVICE)\n",
    "    te_loss, te_acc = evaluate(tf_model, test_dl, criterion, DEVICE)\n",
    "    dt = time.time() - t0\n",
    "    tf_hist[\"train_loss\"].append(tr_loss)\n",
    "    tf_hist[\"test_loss\"].append(te_loss)\n",
    "    tf_hist[\"train_acc\"].append(tr_acc)\n",
    "    tf_hist[\"test_acc\"].append(te_acc)\n",
    "    tf_hist[\"epoch_time\"].append(dt)\n",
    "    if (ep + 1) % 5 == 0:\n",
    "        print(f\"  [TF] Epoch {ep+1:2d}/{N_EPOCHS}: loss={tr_loss:.4f}  \"\n",
    "              f\"acc={tr_acc:.3f}  test_acc={te_acc:.3f}  ({dt:.1f}s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(s4_hist[\"train_loss\"], label=\"S4 train\", linewidth=2)\n",
    "axes[0].plot(s4_hist[\"test_loss\"], \"--\", label=\"S4 test\", linewidth=2)\n",
    "axes[0].plot(tf_hist[\"train_loss\"], label=\"Transformer train\", linewidth=2)\n",
    "axes[0].plot(tf_hist[\"test_loss\"], \"--\", label=\"Transformer test\", linewidth=2)\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].set_title(\"Training & Test Loss\")\n",
    "axes[0].legend(fontsize=8)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(s4_hist[\"test_acc\"], \"o-\", label=\"S4\", markersize=3, linewidth=2)\n",
    "axes[1].plot(tf_hist[\"test_acc\"], \"s-\", label=\"Transformer\", markersize=3, linewidth=2)\n",
    "axes[1].axhline(0.5, color=\"gray\", linestyle=\":\", alpha=0.5, label=\"random\")\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Test Accuracy\")\n",
    "axes[1].set_title(\"Test Accuracy\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim(0.4, 1.05)\n",
    "\n",
    "# Timing\n",
    "labels = [\"S4\", \"Transformer\"]\n",
    "avg_times = [np.mean(s4_hist[\"epoch_time\"]), np.mean(tf_hist[\"epoch_time\"])]\n",
    "colors = [\"steelblue\", \"coral\"]\n",
    "axes[2].bar(labels, avg_times, color=colors, edgecolor=\"k\")\n",
    "axes[2].set_ylabel(\"Avg seconds / epoch\")\n",
    "axes[2].set_title(\"Training Speed\")\n",
    "for i, v in enumerate(avg_times):\n",
    "    axes[2].text(i, v + 0.01, f\"{v:.2f}s\", ha=\"center\", fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"{'Metric':<25} {'S4':>15} {'Transformer':>15}\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Parameters':<25} {s4_params:>15,} {tf_params:>15,}\")\n",
    "print(f\"{'Best test accuracy':<25} {max(s4_hist['test_acc']):>15.3f} {max(tf_hist['test_acc']):>15.3f}\")\n",
    "print(f\"{'Final test accuracy':<25} {s4_hist['test_acc'][-1]:>15.3f} {tf_hist['test_acc'][-1]:>15.3f}\")\n",
    "print(f\"{'Avg epoch time (s)':<25} {np.mean(s4_hist['epoch_time']):>15.2f} {np.mean(tf_hist['epoch_time']):>15.2f}\")\n",
    "print(f\"{'Final test loss':<25} {s4_hist['test_loss'][-1]:>15.4f} {tf_hist['test_loss'][-1]:>15.4f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Discussion\n",
    "\n",
    "### What to expect\n",
    "\n",
    "**S4** should reach near-perfect accuracy quickly because:\n",
    "- The HiPPO-initialized state matrix is specifically designed to retain information over long spans.\n",
    "- The 128-step delay falls well within S4's natural operating range.\n",
    "\n",
    "**The Transformer** can also solve this task, but:\n",
    "- It relies on **positional embeddings + attention** to bridge the 128-step gap.\n",
    "- With only 4 layers and `d_model=64`, it may take longer to converge or plateau at a slightly lower accuracy.\n",
    "- It uses **more parameters** due to the attention weights and feedforward sub-layers.\n",
    "\n",
    "### Key takeaways\n",
    "\n",
    "1. **Long-range tasks favor SSMs.** S4's inductive bias (HiPPO memory) gives it a structural advantage on tasks requiring information retention across many steps.\n",
    "2. **Parameter efficiency.** S4 achieves similar or better accuracy with fewer parameters.\n",
    "3. **Training cost.** S4 uses O(L log L) FFT convolutions; the Transformer uses O(L²) attention. The gap widens with sequence length.\n",
    "\n",
    "### When would the Transformer win?\n",
    "\n",
    "- Tasks requiring **content-based retrieval** (e.g., \"find the token that matches this query\") — attention is explicitly designed for this.\n",
    "- Very **short** sequences where the O(L²) cost is negligible.\n",
    "- Tasks requiring **in-context learning** (e.g., few-shot prompting in LLMs).\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** [04_time_series.ipynb](04_time_series.ipynb) — regression on time-series data, again comparing S4 vs Transformer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
