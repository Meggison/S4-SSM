{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 4 — Time Series Forecasting: S4 vs Transformer\n",
    "\n",
    "In this notebook we tackle a **regression** (forecasting) task and compare **S4** against a **Transformer** baseline.\n",
    "\n",
    "## Task: Multi-variate Sinusoid Forecasting\n",
    "\n",
    "We generate 3-channel sinusoidal signals with varying frequencies, phases, and noise:\n",
    "\n",
    "$$x_c(t) = \\sin(2\\pi f_c t + \\phi_c) + \\epsilon, \\quad c \\in \\{0, 1, 2\\}$$\n",
    "\n",
    "- **Input:** steps $0 \\ldots L{-}1$ (the past)\n",
    "- **Target:** steps $1 \\ldots L$ (one-step-ahead prediction)\n",
    "\n",
    "This tests whether each model can learn the **temporal dynamics** of continuous signals — a natural fit for SSMs, and a case where Transformers must learn temporal structure purely from data.\n",
    "\n",
    "We compare:\n",
    "- **Test MSE**\n",
    "- **Parameter count**\n",
    "- **Training speed**\n",
    "- **Qualitative predictions** (overlay plots)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, time\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), \"..\") if \"tutorials\" in os.getcwd() else os.getcwd())\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sinusoid_dataset(n_samples, seq_len=200, n_features=3, noise_std=0.05):\n",
    "    \"\"\"\n",
    "    Generate multi-channel sinusoidal time series.\n",
    "\n",
    "    Returns:\n",
    "        X: (n_samples, seq_len, n_features)  — input (steps 0..L-1)\n",
    "        Y: (n_samples, seq_len, n_features)  — target (steps 1..L)\n",
    "    \"\"\"\n",
    "    t = np.linspace(0, 4 * np.pi, seq_len + 1)  # one extra step for target\n",
    "    data = np.zeros((n_samples, seq_len + 1, n_features))\n",
    "    for i in range(n_samples):\n",
    "        for c in range(n_features):\n",
    "            freq = np.random.uniform(0.5, 3.0)\n",
    "            phase = np.random.uniform(0, 2 * np.pi)\n",
    "            amp = np.random.uniform(0.5, 1.5)\n",
    "            data[i, :, c] = amp * np.sin(freq * t + phase)\n",
    "    data += np.random.randn(*data.shape) * noise_std\n",
    "\n",
    "    X = torch.tensor(data[:, :-1, :], dtype=torch.float32)\n",
    "    Y = torch.tensor(data[:, 1:, :],  dtype=torch.float32)\n",
    "    return X, Y\n",
    "\n",
    "N_TRAIN, N_TEST = 3000, 500\n",
    "SEQ_LEN = 200\n",
    "N_FEATURES = 3\n",
    "\n",
    "X_train, Y_train = make_sinusoid_dataset(N_TRAIN, SEQ_LEN, N_FEATURES)\n",
    "X_test,  Y_test  = make_sinusoid_dataset(N_TEST,  SEQ_LEN, N_FEATURES)\n",
    "\n",
    "print(f\"Train: X={X_train.shape}, Y={Y_train.shape}\")\n",
    "print(f\"Test:  X={X_test.shape},  Y={Y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a sample\n",
    "idx = 0\n",
    "fig, axes = plt.subplots(N_FEATURES, 1, figsize=(10, 5), sharex=True)\n",
    "for c in range(N_FEATURES):\n",
    "    axes[c].plot(X_train[idx, :, c].numpy(), label=\"Input\", linewidth=1.5)\n",
    "    axes[c].plot(Y_train[idx, :, c].numpy(), \"--\", label=\"Target (shifted by 1)\", linewidth=1.5, alpha=0.7)\n",
    "    axes[c].set_ylabel(f\"Channel {c}\")\n",
    "    if c == 0:\n",
    "        axes[c].legend(fontsize=9)\n",
    "axes[-1].set_xlabel(\"Time step\")\n",
    "fig.suptitle(\"Sample 0: input and one-step-ahead target\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dl = DataLoader(TensorDataset(X_train, Y_train), batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dl  = DataLoader(TensorDataset(X_test, Y_test),   batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Models\n",
    "\n",
    "### S4 model\n",
    "Our `S4SequenceModel` with `task=\"regression\"` — outputs per-timestep predictions without pooling.\n",
    "\n",
    "### Transformer baseline\n",
    "A causal Transformer encoder (with causal mask to prevent future leakage) + positional embeddings → per-step linear projection. Same `d_model` and `n_layers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from s4_lib import S4SequenceModel, get_ssm_param_groups\n",
    "\n",
    "D_MODEL = 64\n",
    "N_LAYERS = 4\n",
    "\n",
    "# ---- S4 model ----\n",
    "s4_model = S4SequenceModel(\n",
    "    d_input=N_FEATURES, d_model=D_MODEL, d_output=N_FEATURES,\n",
    "    n_layers=N_LAYERS, d_state=64, dropout=0.1,\n",
    "    task=\"regression\",\n",
    ").to(DEVICE)\n",
    "\n",
    "s4_params = sum(p.numel() for p in s4_model.parameters())\n",
    "print(f\"S4 model params: {s4_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerForecaster(nn.Module):\n",
    "    \"\"\"Causal Transformer encoder for per-step regression.\"\"\"\n",
    "    def __init__(self, d_input, d_model, n_layers, d_output,\n",
    "                 max_len=512, nhead=4, dim_ff=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_input, d_model)\n",
    "        self.pos_emb = nn.Parameter(torch.randn(1, max_len, d_model) * 0.02)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=dim_ff,\n",
    "            dropout=dropout, batch_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.head = nn.Linear(d_model, d_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L, _ = x.shape\n",
    "        x = self.proj(x) + self.pos_emb[:, :L, :]\n",
    "        # Causal mask — prevent attending to future steps\n",
    "        mask = nn.Transformer.generate_square_subsequent_mask(L, device=x.device)\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        return self.head(x)  # (B, L, d_output)\n",
    "\n",
    "tf_model = TransformerForecaster(\n",
    "    d_input=N_FEATURES, d_model=D_MODEL, n_layers=N_LAYERS, d_output=N_FEATURES,\n",
    "    max_len=SEQ_LEN, nhead=4, dim_ff=D_MODEL*4, dropout=0.1,\n",
    ").to(DEVICE)\n",
    "\n",
    "tf_params = sum(p.numel() for p in tf_model.parameters())\n",
    "print(f\"Transformer model params: {tf_params:,}\")\n",
    "print(f\"Ratio (Transformer / S4): {tf_params/s4_params:.2f}×\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, loader, criterion, device):\n",
    "    model.train()\n",
    "    total_loss, n = 0.0, 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        n += xb.size(0)\n",
    "    return total_loss / n\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, n = 0.0, 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        n += xb.size(0)\n",
    "    return total_loss / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 30\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# S4 optimizer (dual LR)\n",
    "s4_opt = torch.optim.AdamW(\n",
    "    get_ssm_param_groups(s4_model, lr=0.004, ssm_lr=0.001, weight_decay=0.01)\n",
    ")\n",
    "\n",
    "# Transformer optimizer\n",
    "tf_opt = torch.optim.AdamW(tf_model.parameters(), lr=0.004, weight_decay=0.01)\n",
    "\n",
    "# ---- Train S4 ----\n",
    "s4_hist = {\"train_mse\": [], \"test_mse\": [], \"epoch_time\": []}\n",
    "print(\"Training S4...\")\n",
    "for ep in range(N_EPOCHS):\n",
    "    t0 = time.time()\n",
    "    tr_mse = train_one_epoch(s4_model, s4_opt, train_dl, criterion, DEVICE)\n",
    "    te_mse = evaluate(s4_model, test_dl, criterion, DEVICE)\n",
    "    dt = time.time() - t0\n",
    "    s4_hist[\"train_mse\"].append(tr_mse)\n",
    "    s4_hist[\"test_mse\"].append(te_mse)\n",
    "    s4_hist[\"epoch_time\"].append(dt)\n",
    "    if (ep + 1) % 5 == 0:\n",
    "        print(f\"  [S4] Epoch {ep+1:2d}/{N_EPOCHS}: train_MSE={tr_mse:.6f}  \"\n",
    "              f\"test_MSE={te_mse:.6f}  ({dt:.1f}s)\")\n",
    "\n",
    "# ---- Train Transformer ----\n",
    "tf_hist = {\"train_mse\": [], \"test_mse\": [], \"epoch_time\": []}\n",
    "print(\"\\nTraining Transformer...\")\n",
    "for ep in range(N_EPOCHS):\n",
    "    t0 = time.time()\n",
    "    tr_mse = train_one_epoch(tf_model, tf_opt, train_dl, criterion, DEVICE)\n",
    "    te_mse = evaluate(tf_model, test_dl, criterion, DEVICE)\n",
    "    dt = time.time() - t0\n",
    "    tf_hist[\"train_mse\"].append(tr_mse)\n",
    "    tf_hist[\"test_mse\"].append(te_mse)\n",
    "    tf_hist[\"epoch_time\"].append(dt)\n",
    "    if (ep + 1) % 5 == 0:\n",
    "        print(f\"  [TF] Epoch {ep+1:2d}/{N_EPOCHS}: train_MSE={tr_mse:.6f}  \"\n",
    "              f\"test_MSE={te_mse:.6f}  ({dt:.1f}s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# MSE curves\n",
    "axes[0].semilogy(s4_hist[\"train_mse\"], label=\"S4 train\", linewidth=2)\n",
    "axes[0].semilogy(s4_hist[\"test_mse\"], \"--\", label=\"S4 test\", linewidth=2)\n",
    "axes[0].semilogy(tf_hist[\"train_mse\"], label=\"TF train\", linewidth=2)\n",
    "axes[0].semilogy(tf_hist[\"test_mse\"], \"--\", label=\"TF test\", linewidth=2)\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"MSE (log)\")\n",
    "axes[0].set_title(\"Training & Test MSE\")\n",
    "axes[0].legend(fontsize=8)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Final test MSE bar\n",
    "labels = [\"S4\", \"Transformer\"]\n",
    "final_mse = [s4_hist[\"test_mse\"][-1], tf_hist[\"test_mse\"][-1]]\n",
    "colors = [\"steelblue\", \"coral\"]\n",
    "axes[1].bar(labels, final_mse, color=colors, edgecolor=\"k\")\n",
    "axes[1].set_ylabel(\"Test MSE\")\n",
    "axes[1].set_title(\"Final Test MSE\")\n",
    "for i, v in enumerate(final_mse):\n",
    "    axes[1].text(i, v + 0.0001, f\"{v:.5f}\", ha=\"center\", fontsize=10)\n",
    "\n",
    "# Timing\n",
    "avg_times = [np.mean(s4_hist[\"epoch_time\"]), np.mean(tf_hist[\"epoch_time\"])]\n",
    "axes[2].bar(labels, avg_times, color=colors, edgecolor=\"k\")\n",
    "axes[2].set_ylabel(\"Avg seconds / epoch\")\n",
    "axes[2].set_title(\"Training Speed\")\n",
    "for i, v in enumerate(avg_times):\n",
    "    axes[2].text(i, v + 0.01, f\"{v:.2f}s\", ha=\"center\", fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Qualitative Predictions\n",
    "\n",
    "Let's overlay the model predictions on a few test samples to visually assess how well each model tracks the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s4_model.eval()\n",
    "tf_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    s4_preds = s4_model(X_test[:4].to(DEVICE)).cpu()\n",
    "    tf_preds = tf_model(X_test[:4].to(DEVICE)).cpu()\n",
    "\n",
    "fig, axes = plt.subplots(4, N_FEATURES, figsize=(14, 10), sharex=True)\n",
    "for row in range(4):\n",
    "    for c in range(N_FEATURES):\n",
    "        ax = axes[row, c]\n",
    "        ax.plot(Y_test[row, :, c].numpy(), \"k-\", label=\"Ground truth\", linewidth=1.5, alpha=0.6)\n",
    "        ax.plot(s4_preds[row, :, c].numpy(), \"-\", label=\"S4\", linewidth=1.5)\n",
    "        ax.plot(tf_preds[row, :, c].numpy(), \"--\", label=\"Transformer\", linewidth=1.5)\n",
    "        if row == 0:\n",
    "            ax.set_title(f\"Channel {c}\")\n",
    "        if c == 0:\n",
    "            ax.set_ylabel(f\"Sample {row}\")\n",
    "        if row == 0 and c == 0:\n",
    "            ax.legend(fontsize=7)\n",
    "axes[-1, 1].set_xlabel(\"Time step\")\n",
    "fig.suptitle(\"Predictions vs Ground Truth (4 test samples)\", fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"{'Metric':<25} {'S4':>15} {'Transformer':>15}\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Parameters':<25} {s4_params:>15,} {tf_params:>15,}\")\n",
    "print(f\"{'Best test MSE':<25} {min(s4_hist['test_mse']):>15.6f} {min(tf_hist['test_mse']):>15.6f}\")\n",
    "print(f\"{'Final test MSE':<25} {s4_hist['test_mse'][-1]:>15.6f} {tf_hist['test_mse'][-1]:>15.6f}\")\n",
    "print(f\"{'Avg epoch time (s)':<25} {np.mean(s4_hist['epoch_time']):>15.2f} {np.mean(tf_hist['epoch_time']):>15.2f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Discussion\n",
    "\n",
    "### Why SSMs are a natural fit for time series\n",
    "\n",
    "Time-series data is inherently **continuous and sequential**. State space models were originally developed in control theory for exactly this kind of signal:\n",
    "\n",
    "1. **Continuous-time formulation:** The SSM's underlying ODE $x' = Ax + Bu$ directly models continuous dynamics. The learnable step size $\\Delta$ adapts to the signal's natural frequency.\n",
    "\n",
    "2. **Causal by construction:** SSMs process data left-to-right through state updates — no need for causal masks or tricks.\n",
    "\n",
    "3. **Efficient long-horizon forecasting:** In RNN mode, S4 generates one step at a time with O(1) cost per step and constant memory.\n",
    "\n",
    "### How the Transformer approaches this differently\n",
    "\n",
    "- The Transformer has **no notion of continuous time** — it treats the sequence as a bag of positions and must learn temporal structure from positional embeddings.\n",
    "- The causal mask restricts attention to the past, but each step still requires O(L) computation to attend over the full context.\n",
    "- Transformers can still perform well when given enough capacity and data, but they lack the **inductive bias** for smooth, continuous signals.\n",
    "\n",
    "### When would a Transformer outperform S4 on time series?\n",
    "\n",
    "- **Irregular time series** with complex event-driven patterns (e.g., financial trades, clinical events)\n",
    "- **Multi-modal time series** where cross-channel attention patterns matter\n",
    "- When combined with time-series-specific Transformer variants (PatchTST, Informer, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "## Congratulations!\n",
    "\n",
    "You've completed all four tutorials. You now know:\n",
    "1. The **theory** behind SSMs, HiPPO, and S4\n",
    "2. How to **use the s4_lib library** in practice\n",
    "3. How S4 compares to Transformers on **classification** and **time-series** tasks\n",
    "\n",
    "For further exploration:\n",
    "- Try increasing the sequence length to 1000+ — watch the S4/Transformer gap widen\n",
    "- Experiment with S4's `d_state` parameter — more state = longer memory\n",
    "- Read the [S4 paper](https://arxiv.org/abs/2111.00396) and [S4D paper](https://arxiv.org/abs/2206.11893) for the full mathematical details"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
