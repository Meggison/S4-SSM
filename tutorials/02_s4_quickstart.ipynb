{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2 — S4 Quickstart\n",
    "\n",
    "A hands-on introduction to using the **s4_lib** library. You'll learn:\n",
    "\n",
    "1. **Import & Instantiate** — Create S4 and S4D layers / full models\n",
    "2. **Forward Pass** — Process a batch of sequences in CNN (parallel) mode\n",
    "3. **RNN Stepping** — Auto-regressive / streaming inference\n",
    "4. **Optimizer Setup** — The special learning-rate treatment S4 requires\n",
    "5. **Training Loop** — A minimal training step\n",
    "6. **Parameter Comparison** — S4 vs a Transformer encoder of equivalent capacity\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), \"..\") if \"tutorials\" in os.getcwd() else os.getcwd())\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import & Instantiate\n",
    "\n",
    "The library provides three main building blocks:\n",
    "\n",
    "| Class | Description |\n",
    "|-------|-------------|\n",
    "| `S4Layer` | Full DPLR S4 layer (uses Cauchy kernel) — one layer of the original S4 |\n",
    "| `S4DLayer` | Diagonal S4 variant (simpler, nearly as good) |\n",
    "| `S4DBlock` | S4D + normalization + GELU + residual + dropout — a \"Transformer block\" analogue |\n",
    "| `S4SequenceModel` | Complete model: embedding → stacked S4DBlocks → head (classification or regression) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from s4_lib import S4Layer, S4DLayer, S4DBlock, S4SequenceModel\n",
    "\n",
    "# -- Single S4 layer (DPLR) --\n",
    "s4_layer = S4Layer(d_model=64, state_dim=64, dt_min=0.001, dt_max=0.1)\n",
    "print(\"S4Layer:\")\n",
    "print(f\"  d_model=64, state_dim=64\")\n",
    "print(f\"  params: {sum(p.numel() for p in s4_layer.parameters()):,}\")\n",
    "\n",
    "# -- Single S4D layer (diagonal) --\n",
    "s4d_layer = S4DLayer(d_model=64, state_dim=64)\n",
    "print(f\"\\nS4DLayer:\")\n",
    "print(f\"  params: {sum(p.numel() for p in s4d_layer.parameters()):,}\")\n",
    "\n",
    "# -- Full model for classification --\n",
    "model = S4SequenceModel(\n",
    "    d_input=3,          # input features per time step\n",
    "    d_model=64,         # hidden dimension\n",
    "    d_state=64,         # SSM state dimension\n",
    "    n_layers=4,         # number of S4D blocks\n",
    "    d_output=10,        # classification head (num classes)\n",
    "    dropout=0.1,\n",
    "    task=\"classification\",  # mean-pool over time → single vector → classifier\n",
    ")\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nS4SequenceModel (4 layers, d=64):\")\n",
    "print(f\"  Total params: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Forward Pass (CNN Mode)\n",
    "\n",
    "In **CNN mode**, the S4 layer materializes its convolution kernel of length $L$ and applies it via FFT. This is fully parallelizable — like running a 1-D convolution over the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, seq_len, features = 8, 256, 3\n",
    "x = torch.randn(batch, seq_len, features)\n",
    "\n",
    "# Full model forward\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(x)  # (batch, n_classes)\n",
    "\n",
    "print(f\"Input:  {x.shape}  →  (batch, seq_len, features)\")\n",
    "print(f\"Output: {logits.shape}  →  (batch, n_classes)\")\n",
    "print(f\"\\nPredicted classes: {logits.argmax(dim=-1).tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual layer forward — returns (batch, seq_len, d_model)\n",
    "x_proj = nn.Linear(features, 64)(x)  # project to d_model\n",
    "with torch.no_grad():\n",
    "    y_s4d = s4d_layer(x_proj)\n",
    "print(f\"S4DLayer: {x_proj.shape} → {y_s4d.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RNN Stepping (Auto-regressive / Streaming Inference)\n",
    "\n",
    "For generation or streaming applications, S4 can switch to **RNN mode** — processing one time step at a time with constant memory:\n",
    "\n",
    "```\n",
    "state = initial_state()          # shape: (batch, d_model, state_dim)\n",
    "for t in range(seq_len):\n",
    "    y_t, state = layer.step(u_t, state)  # O(1) per step\n",
    "```\n",
    "\n",
    "The key advantage: **constant memory** regardless of sequence length. A Transformer would need to keep the full KV cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN stepping with S4DLayer\n",
    "batch_rnn = 2\n",
    "x_rnn = torch.randn(batch_rnn, 32, 64)  # (batch, time, d_model)\n",
    "\n",
    "# Step-by-step\n",
    "state = s4d_layer.init_state(batch_rnn)  # zero state\n",
    "outputs_rnn = []\n",
    "for t in range(32):\n",
    "    y_t, state = s4d_layer.step(x_rnn[:, t, :], state)\n",
    "    outputs_rnn.append(y_t)\n",
    "y_rnn = torch.stack(outputs_rnn, dim=1)\n",
    "\n",
    "# Compare with CNN forward\n",
    "with torch.no_grad():\n",
    "    y_cnn = s4d_layer(x_rnn)\n",
    "\n",
    "diff = (y_rnn - y_cnn).abs().max().item()\n",
    "print(f\"RNN output shape: {y_rnn.shape}\")\n",
    "print(f\"CNN output shape: {y_cnn.shape}\")\n",
    "print(f\"Max difference:   {diff:.2e}  ({'✓ Match' if diff < 1e-3 else '✗ Mismatch'})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optimizer Setup\n",
    "\n",
    "S4 has a quirk: the SSM parameters ($A$, $B$, $\\Delta$) need a **different learning rate** than the rest of the model (typically higher, e.g., `0.001` for SSM params vs `0.004` for others).\n",
    "\n",
    "The library provides `get_ssm_param_groups` to set this up automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from s4_lib import get_ssm_param_groups\n",
    "\n",
    "param_groups = get_ssm_param_groups(model, lr=0.004, ssm_lr=0.001, weight_decay=0.01)\n",
    "\n",
    "for i, g in enumerate(param_groups):\n",
    "    n_params = sum(p.numel() for p in g[\"params\"])\n",
    "    print(f\"Group {i}: lr={g['lr']}, weight_decay={g['weight_decay']}, params={n_params:,}\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(param_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Minimal Training Step\n",
    "\n",
    "Let's run a quick training step on random data to verify everything works end-to-end:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Fake data\n",
    "x_train = torch.randn(16, 256, 3)\n",
    "y_train = torch.randint(0, 10, (16,))\n",
    "\n",
    "losses = []\n",
    "for step in range(20):\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(x_train)\n",
    "    loss = criterion(logits, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "    if step % 5 == 0:\n",
    "        print(f\"Step {step:2d}: loss = {loss.item():.4f}\")\n",
    "\n",
    "print(f\"\\nLoss decreased: {losses[0]:.4f} → {losses[-1]:.4f}  ({'✓' if losses[-1] < losses[0] else '✗'})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 3.5))\n",
    "ax.plot(losses, \"o-\", markersize=4)\n",
    "ax.set_xlabel(\"Step\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_title(\"Training loss (random data — just verifying the pipeline works)\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Parameter Comparison: S4 vs Transformer\n",
    "\n",
    "A key selling point of S4 is **parameter efficiency**. Let's build a Transformer encoder with the same hidden dimension and number of layers, then compare total parameter counts on identical classification tasks.\n",
    "\n",
    "We'll fix:\n",
    "- `d_model = 64`, `n_layers = 4`, `n_classes = 10`, `d_input = 3`\n",
    "\n",
    "For the Transformer, we use `nn.TransformerEncoder` with `nhead=4` and `dim_feedforward=256` (4× expansion, the standard ratio)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "    \"\"\"Simple Transformer encoder + mean-pool + classifier.\"\"\"\n",
    "    def __init__(self, d_input, d_model, n_layers, n_classes, nhead=4, dim_ff=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_input, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=dim_ff,\n",
    "            dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.head = nn.Linear(d_model, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        x = self.encoder(x)\n",
    "        x = x.mean(dim=1)  # pool over time\n",
    "        return self.head(x)\n",
    "\n",
    "\n",
    "# Instantiate both\n",
    "s4_model = S4SequenceModel(d_input=3, d_model=64, d_state=64, n_layers=4,\n",
    "                           d_output=10, dropout=0.1, task=\"classification\")\n",
    "tf_model = TransformerClassifier(d_input=3, d_model=64, n_layers=4,\n",
    "                                 n_classes=10, nhead=4, dim_ff=256, dropout=0.1)\n",
    "\n",
    "s4_params = sum(p.numel() for p in s4_model.parameters())\n",
    "tf_params = sum(p.numel() for p in tf_model.parameters())\n",
    "\n",
    "print(\"=\" * 55)\n",
    "print(f\"{'Model':<25} {'Params':>12} {'Ratio':>10}\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"{'S4SequenceModel (4L)':<25} {s4_params:>12,}      1.0×\")\n",
    "print(f\"{'TransformerEncoder (4L)':<25} {tf_params:>12,}    {tf_params/s4_params:>5.1f}×\")\n",
    "print(\"=\" * 55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breakdown by component\n",
    "def param_breakdown(model, name):\n",
    "    breakdown = {}\n",
    "    for n, p in model.named_parameters():\n",
    "        component = n.split(\".\")[0]\n",
    "        breakdown[component] = breakdown.get(component, 0) + p.numel()\n",
    "    print(f\"\\n{name} breakdown:\")\n",
    "    for k, v in sorted(breakdown.items()):\n",
    "        print(f\"  {k:<20} {v:>8,}\")\n",
    "\n",
    "param_breakdown(s4_model, \"S4SequenceModel\")\n",
    "param_breakdown(tf_model, \"TransformerClassifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling comparison: vary d_model\n",
    "dims = [32, 64, 128, 256]\n",
    "s4_counts, tf_counts = [], []\n",
    "\n",
    "for d in dims:\n",
    "    s4_m = S4SequenceModel(d_input=3, d_model=d, d_state=64, n_layers=4,\n",
    "                           d_output=10, dropout=0.0, task=\"classification\")\n",
    "    tf_m = TransformerClassifier(d_input=3, d_model=d, n_layers=4,\n",
    "                                n_classes=10, nhead=4, dim_ff=d*4, dropout=0.0)\n",
    "    s4_counts.append(sum(p.numel() for p in s4_m.parameters()))\n",
    "    tf_counts.append(sum(p.numel() for p in tf_m.parameters()))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Absolute counts\n",
    "axes[0].bar([str(d) for d in dims], s4_counts, width=0.35, label=\"S4\", alpha=0.85)\n",
    "axes[0].bar([str(d) for d in dims], tf_counts, width=0.35, label=\"Transformer\",\n",
    "            alpha=0.85, bottom=None)\n",
    "x_pos = np.arange(len(dims))\n",
    "w = 0.35\n",
    "axes[0].cla()\n",
    "axes[0].bar(x_pos - w/2, [c/1000 for c in s4_counts], w, label=\"S4\", color=\"steelblue\")\n",
    "axes[0].bar(x_pos + w/2, [c/1000 for c in tf_counts], w, label=\"Transformer\", color=\"coral\")\n",
    "axes[0].set_xticks(x_pos)\n",
    "axes[0].set_xticklabels([str(d) for d in dims])\n",
    "axes[0].set_xlabel(\"d_model\")\n",
    "axes[0].set_ylabel(\"Params (thousands)\")\n",
    "axes[0].set_title(\"Total Parameters\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Ratio\n",
    "ratios = [t / s for s, t in zip(s4_counts, tf_counts)]\n",
    "axes[1].plot(dims, ratios, \"o-\", color=\"purple\", linewidth=2, markersize=8)\n",
    "axes[1].axhline(1, color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
    "axes[1].set_xlabel(\"d_model\")\n",
    "axes[1].set_ylabel(\"Transformer / S4 param ratio\")\n",
    "axes[1].set_title(\"Parameter Ratio (>1 means Transformer is bigger)\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nParameter ratio (Transformer / S4) by d_model:\")\n",
    "for d, r in zip(dims, ratios):\n",
    "    print(f\"  d_model={d:<4}: {r:.2f}×\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key takeaways\n",
    "\n",
    "- The Transformer's feedforward sub-layers (two dense matrices per layer, $d \\to 4d \\to d$) account for the bulk of its parameters.\n",
    "- S4 layers are **much more parameter-efficient** because the SSM parameters ($\\Lambda$, $B$, $C$, $\\log\\Delta$) only depend on the state dimension $N$, not on $d_{\\text{model}}^2$.\n",
    "- As `d_model` grows, the gap widens — making S4 increasingly attractive for large-scale settings.\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** [03_classification.ipynb](03_classification.ipynb) — train S4 and a Transformer on a real classification task and compare accuracy, speed, and parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
