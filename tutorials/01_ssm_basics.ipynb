{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1 — SSM Basics\n",
    "\n",
    "This notebook walks through the foundational concepts behind **S4** (Structured State Spaces for Sequence Modeling). By the end, you'll understand:\n",
    "\n",
    "1. **What is a State Space Model (SSM)?** — The continuous-time dynamical system at the heart of S4\n",
    "2. **The HiPPO Matrix** — Why the initialization of the state matrix *A* is the secret sauce\n",
    "3. **Discretization** — How we bridge the continuous math to discrete sequences (text, audio, pixels)\n",
    "4. **CNN ↔ RNN Duality** — How S4 trains like a CNN but infers like an RNN\n",
    "5. **SSMs vs Transformers** — Where SSMs shine compared to attention-based LLMs\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), \"..\") if \"tutorials\" in os.getcwd() else os.getcwd())\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is a State Space Model?\n",
    "\n",
    "A **State Space Model (SSM)** is a linear dynamical system that maps an input signal $u(t)$ to an output $y(t)$ through a latent state $x(t)$:\n",
    "\n",
    "$$\n",
    "x'(t) = A\\, x(t) + B\\, u(t) \\qquad \\text{(state equation)}\n",
    "$$\n",
    "$$\n",
    "y(t) = C\\, x(t) + D\\, u(t) \\qquad \\text{(output equation)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $A \\in \\mathbb{R}^{N \\times N}$ — **state matrix** (controls the dynamics — this is the key!)\n",
    "- $B \\in \\mathbb{R}^{N \\times 1}$ — input-to-state projection\n",
    "- $C \\in \\mathbb{R}^{1 \\times N}$ — state-to-output projection\n",
    "- $D \\in \\mathbb{R}$ — skip / direct feed-through connection\n",
    "- $N$ — state dimension (how much \"memory\" the model has)\n",
    "\n",
    "### Why does this matter for deep learning?\n",
    "\n",
    "Think of an SSM as a **learnable filter**: it reads input one step at a time, updates a compressed memory (the state), and produces output. This is similar to an RNN — but with a crucial difference: **the structure of $A$ can be exploited for efficient parallel computation**.\n",
    "\n",
    "The **key insight** of S4: a *random* $A$ gives poor long-range performance. But a specially designed $A$ (the **HiPPO matrix**) lets the state optimally approximate the entire input history using polynomial projections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The HiPPO Matrix\n",
    "\n",
    "**HiPPO** (High-order Polynomial Projection Operators) provides a principled way to initialize the state matrix $A$.\n",
    "\n",
    "The idea: at each time $t$, the state $x(t)$ should store the **optimal polynomial approximation** of the input history $u(s)$ for $s \\leq t$. The HiPPO-LegS variant uses **scaled Legendre polynomials** as the basis.\n",
    "\n",
    "The matrix entries are:\n",
    "$$\n",
    "A_{nk} = \\begin{cases}\n",
    "-\\sqrt{(2n+1)(2k+1)} & \\text{if } n > k \\\\\n",
    "-(n+1) & \\text{if } n = k \\\\\n",
    "0 & \\text{if } n < k\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This makes $A$ **lower-triangular** with **all eigenvalues having negative real parts** (guaranteed stability).\n",
    "\n",
    "Let's visualize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from s4_lib.hippo import make_hippo_legs, make_hippo_b, diagonal_init\n",
    "\n",
    "N = 16  # small state dim for visualization\n",
    "A = make_hippo_legs(N)\n",
    "B = make_hippo_b(N, \"legs\")\n",
    "\n",
    "print(f\"HiPPO-LegS matrix A shape: {A.shape}\")\n",
    "print(f\"A is lower-triangular: {np.allclose(A, np.tril(A))}\")\n",
    "\n",
    "eigs = np.linalg.eigvals(A)\n",
    "print(f\"\\nEigenvalue real parts: [{eigs.real.min():.2f}, {eigs.real.max():.2f}]\")\n",
    "print(f\"All stable (Re < 0): {(eigs.real < 0).all()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4.5))\n",
    "\n",
    "# Matrix heatmap\n",
    "im = axes[0].imshow(A, cmap=\"RdBu_r\", vmin=-abs(A).max(), vmax=abs(A).max())\n",
    "axes[0].set_title(\"HiPPO-LegS Matrix (N=16)\", fontsize=13)\n",
    "axes[0].set_xlabel(\"Column k\")\n",
    "axes[0].set_ylabel(\"Row n\")\n",
    "plt.colorbar(im, ax=axes[0], shrink=0.8)\n",
    "\n",
    "# Eigenvalue plot\n",
    "axes[1].scatter(eigs.real, eigs.imag, c=\"steelblue\", edgecolors=\"k\", s=60, zorder=5)\n",
    "axes[1].axvline(0, color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
    "axes[1].set_title(\"Eigenvalues of HiPPO-LegS\", fontsize=13)\n",
    "axes[1].set_xlabel(\"Real part\")\n",
    "axes[1].set_ylabel(\"Imaginary part\")\n",
    "axes[1].annotate(\"All eigenvalues in\\nleft half-plane → stable\",\n",
    "                 xy=(-8, 0), fontsize=10, color=\"steelblue\",\n",
    "                 bbox=dict(boxstyle=\"round\", fc=\"lightyellow\", alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key takeaway:** The HiPPO matrix gives SSMs a mathematical mechanism for capturing **long-range dependencies** — something that RNNs (which use random init) famously struggle with, and Transformers solve with quadratic-cost attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Discretization\n",
    "\n",
    "The continuous SSM operates on continuous signals, but real-world data (text tokens, audio samples, image pixels) is **discrete**. We need to **discretize** the system.\n",
    "\n",
    "Given a learnable step size $\\Delta$, we convert $(A, B) \\to (\\bar{A}, \\bar{B})$:\n",
    "\n",
    "| Method | $\\bar{A}$ | $\\bar{B}$ | Used in |\n",
    "|--------|-----------|-----------|--------|\n",
    "| **Bilinear** (Tustin) | $(I - \\frac{\\Delta}{2}A)^{-1}(I + \\frac{\\Delta}{2}A)$ | $(I - \\frac{\\Delta}{2}A)^{-1} \\Delta B$ | Original S4 |\n",
    "| **Zero-Order Hold** | $e^{\\Delta A}$ | $A^{-1}(e^{\\Delta A} - I)B$ | S4D, Mamba |\n",
    "\n",
    "The discrete system then becomes a standard recurrence:\n",
    "$$x[k] = \\bar{A}\\, x[k-1] + \\bar{B}\\, u[k], \\qquad y[k] = C\\, x[k] + D\\, u[k]$$\n",
    "\n",
    "The step size $\\Delta$ is **learnable** — it controls the *resolution* at which the model \"sees\" the input (analogous to the receptive field in CNNs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from s4_lib.discretization import bilinear, zoh\n",
    "\n",
    "# Example: discretize a 2D diagonal system\n",
    "Lambda = torch.tensor([-1.0 + 2j, -3.0 - 1j], dtype=torch.cfloat)\n",
    "B_ex = torch.tensor([1.0 + 0j, 1.0 + 0j], dtype=torch.cfloat)\n",
    "dt = torch.tensor(0.01)\n",
    "\n",
    "Ab_bil, Bb_bil = bilinear(Lambda, B_ex, dt)\n",
    "Ab_zoh, Bb_zoh = zoh(Lambda, B_ex, dt)\n",
    "\n",
    "print(\"Continuous eigenvalues:\")\n",
    "for i, l in enumerate(Lambda):\n",
    "    print(f\"  λ{i} = {l.item():.4f}  (|λ| = {abs(l):.4f})\")\n",
    "\n",
    "print(f\"\\nAfter discretization (Δ = {dt.item()}):\\n\")\n",
    "print(f\"  Bilinear |Ā|: {[f'{abs(a):.6f}' for a in Ab_bil.tolist()]}\")\n",
    "print(f\"  ZOH      |Ā|: {[f'{abs(a):.6f}' for a in Ab_zoh.tolist()]}\")\n",
    "print(f\"\\n  All |Ā| < 1 (stable): Bilinear={all(abs(a)<1 for a in Ab_bil.tolist())}, \"\n",
    "      f\"ZOH={all(abs(a)<1 for a in Ab_zoh.tolist())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. CNN ↔ RNN Duality\n",
    "\n",
    "The discrete SSM can be viewed in **two equivalent ways**:\n",
    "\n",
    "### RNN view (sequential) — good for inference\n",
    "```python\n",
    "for k in range(L):\n",
    "    x[k] = A_bar * x[k-1] + B_bar * u[k]\n",
    "    y[k] = C * x[k]\n",
    "```\n",
    "Process one step at a time: $O(1)$ memory per step.\n",
    "\n",
    "### CNN view (parallel) — good for training\n",
    "Unroll the recurrence to get a **convolution kernel**:\n",
    "$$K = (C\\bar{B},\\; C\\bar{A}\\bar{B},\\; C\\bar{A}^2\\bar{B},\\; \\ldots,\\; C\\bar{A}^{L-1}\\bar{B})$$\n",
    "$$y = u * K \\qquad \\text{(convolution, computed via FFT in } O(L \\log L) \\text{)}$$\n",
    "\n",
    "**S4's key contribution:** compute $K$ efficiently using the DPLR structure of $A$ and Cauchy kernels, avoiding the $O(N^2 L)$ naive cost.\n",
    "\n",
    "Let's verify that both views produce **identical output**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from s4_lib.hippo import s4d_inv_init\n",
    "from s4_lib.kernels import kernel_diagonal, fft_conv\n",
    "\n",
    "N_demo, L_demo = 8, 64\n",
    "Lambda_d = torch.from_numpy(s4d_inv_init(N_demo))\n",
    "B_d = torch.randn(N_demo, dtype=torch.cfloat) * 0.1\n",
    "C_d = torch.randn(N_demo, dtype=torch.cfloat) * 0.1\n",
    "dt_d = torch.tensor(0.05)\n",
    "\n",
    "# ---- CNN mode: compute kernel, convolve ----\n",
    "K = kernel_diagonal(Lambda_d, B_d, C_d, dt_d, L_demo, method=\"zoh\")\n",
    "u = torch.randn(1, L_demo)\n",
    "y_cnn = fft_conv(u, K)\n",
    "\n",
    "# ---- RNN mode: step-by-step ----\n",
    "Ab_d = torch.exp(Lambda_d * dt_d)\n",
    "Bb_d = B_d * (Ab_d - 1.0) / Lambda_d\n",
    "state = torch.zeros(N_demo, dtype=torch.cfloat)\n",
    "y_rnn = []\n",
    "for k in range(L_demo):\n",
    "    state = Ab_d * state + Bb_d * u[0, k]\n",
    "    y_rnn.append((C_d * state).sum().real.item())\n",
    "y_rnn = torch.tensor(y_rnn).unsqueeze(0)\n",
    "\n",
    "error = (y_cnn - y_rnn).abs().max().item()\n",
    "print(f\"Max absolute difference between CNN and RNN outputs: {error:.2e}\")\n",
    "print(f\"Equivalent: {error < 1e-4} ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 3.5))\n",
    "ax.plot(y_cnn[0].detach().numpy(), label=\"CNN mode (parallel)\", linewidth=2)\n",
    "ax.plot(y_rnn[0].detach().numpy(), \"--\", label=\"RNN mode (sequential)\", linewidth=2)\n",
    "ax.set_xlabel(\"Time step\")\n",
    "ax.set_ylabel(\"Output\")\n",
    "ax.set_title(\"CNN and RNN modes produce identical output\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SSMs vs Transformers / LLMs\n",
    "\n",
    "Both S4/SSMs and Transformers are **sequence models**, but they take fundamentally different approaches:\n",
    "\n",
    "| | **Transformer (GPT, BERT, etc.)** | **S4 / SSM** |\n",
    "|---|---|---|\n",
    "| **Core mechanism** | Self-attention: every token attends to every other token | State recurrence: compress history into a fixed-size state |\n",
    "| **Training complexity** | $O(L^2 d)$ — quadratic in sequence length | $O(L \\log L \\cdot N)$ — near-linear via FFT |\n",
    "| **Inference (per step)** | $O(L \\cdot d)$ — must attend to full KV cache | $O(N \\cdot d)$ — constant, just update state |\n",
    "| **Memory at inference** | KV cache grows linearly with context | Fixed $O(N \\cdot d)$ state |\n",
    "| **Long sequences** | Struggles beyond training length (or needs tricks like RoPE, ALiBi) | Handles 10K–100K+ steps naturally |\n",
    "| **Strengths** | In-context learning, flexible attention patterns | Long-range dependencies, continuous-time signals, efficiency |\n",
    "\n",
    "### When to use each:\n",
    "\n",
    "- **Transformers** excel at tasks where **flexible, content-dependent routing** matters (e.g., language understanding, in-context learning, retrieval).\n",
    "- **SSMs/S4** excel at tasks with **very long sequences** or **continuous signals** (audio, time series, genomics, long-range arena benchmarks).\n",
    "\n",
    "Modern architectures like **Mamba** (Gu & Dao, 2023) combine SSM efficiency with input-dependent *selectivity*, approaching Transformer quality on language while keeping linear scaling.\n",
    "\n",
    "### Key numbers from the S4 paper:\n",
    "- **91%** on sequential CIFAR-10 (pixel-by-pixel, length 1024) — matching 2D ResNets\n",
    "- **86%** average on Long Range Arena (length 1K–16K) — all prior models < 60%\n",
    "- **60× faster** generation than Transformers on language modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick complexity comparison\n",
    "seq_lengths = [128, 256, 512, 1024, 2048, 4096, 8192, 16384]\n",
    "d = 256  # model dim\n",
    "N = 64   # state dim\n",
    "\n",
    "transformer_flops = [L * L * d for L in seq_lengths]       # O(L^2 d)\n",
    "s4_flops          = [L * np.log2(L) * N for L in seq_lengths]  # O(L log L * N)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4.5))\n",
    "ax.semilogy(seq_lengths, transformer_flops, \"o-\", label=\"Transformer: O(L² d)\", linewidth=2)\n",
    "ax.semilogy(seq_lengths, s4_flops, \"s-\", label=\"S4: O(L log L · N)\", linewidth=2)\n",
    "ax.set_xlabel(\"Sequence length L\")\n",
    "ax.set_ylabel(\"Relative FLOPs (log scale)\")\n",
    "ax.set_title(\"Training Cost: Transformer vs S4\")\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"At L=16,384:  Transformer is {transformer_flops[-1]/s4_flops[-1]:.0f}× more expensive than S4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| Concept | Key idea |\n",
    "|---------|----------|\n",
    "| **SSM** | Linear dynamical system: $x' = Ax + Bu$, $y = Cx + Du$ |\n",
    "| **HiPPO** | Special $A$ matrix that optimally compresses input history |\n",
    "| **Discretization** | Convert continuous $(A,B) \\to$ discrete $(\\bar A, \\bar B)$ with learnable step $\\Delta$ |\n",
    "| **CNN/RNN duality** | Train via FFT convolution (parallel), generate via recurrence (O(1)/step) |\n",
    "| **vs Transformers** | S4 scales linearly with sequence length; Transformers scale quadratically |\n",
    "\n",
    "**Next:** [02_s4_quickstart.ipynb](02_s4_quickstart.ipynb) — hands-on usage of the library."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
